## Quantization backend
	- **FP16 / BF16**
	- **INT8 (AWQ)**
	- **INT4 (AWQ)**
	- **GPTQ** (limited but improving)
	- Many HF “quantized” models use **bitsandbytes**: