## Quantization backend
	- **FP16 / BF16**
	- **INT8 (AWQ)**
	- **INT4 (AWQ)**
	- **GPTQ** (limited but improving)
	- Many HF â€œquantizedâ€ models use **bitsandbytes**:
- ## Notes
	- vLLM un-compatible:
		- load_in_4bit=True
			- This relies on:
			- PyTorch custom ops
			- runtime dequantization
			- ğŸš« vLLM does not use PyTorch forward()
			  ğŸš« vLLM cannot intercept bitsandbytes ops