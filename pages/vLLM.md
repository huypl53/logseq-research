## Concepts
	- KV cache
	- ## [[PagedAttention]]
		-
	- ## FlashAttention
- ## Kernels
	- for quantization compatible
- ## Q&A
	- does KV-cache work on VL models?
	- how about quantizing
	- how to self-quantize?
	-